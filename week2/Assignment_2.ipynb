{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "\\- Si Nguyen Mai, May 19, 2018 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, RFE\n",
    "\n",
    "from nltk import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>energy</th>\n",
       "      <th>liveliness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>duration</th>\n",
       "      <th>loudness</th>\n",
       "      <th>valence</th>\n",
       "      <th>danceability</th>\n",
       "      <th>mode</th>\n",
       "      <th>time_signature_confidence</th>\n",
       "      <th>tempo_confidence</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode_confidence</th>\n",
       "      <th>moods</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.831914</td>\n",
       "      <td>0.056928</td>\n",
       "      <td>106.245</td>\n",
       "      <td>0.232162</td>\n",
       "      <td>0.072076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.60000</td>\n",
       "      <td>-5.984</td>\n",
       "      <td>0.555338</td>\n",
       "      <td>0.837518</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.000</td>\n",
       "      <td>motivational</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434896</td>\n",
       "      <td>0.113409</td>\n",
       "      <td>105.276</td>\n",
       "      <td>0.071236</td>\n",
       "      <td>0.456392</td>\n",
       "      <td>0.322574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>242.21333</td>\n",
       "      <td>-13.902</td>\n",
       "      <td>0.290687</td>\n",
       "      <td>0.459532</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.907</td>\n",
       "      <td>classy</td>\n",
       "      <td>jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.326645</td>\n",
       "      <td>0.843938</td>\n",
       "      <td>123.949</td>\n",
       "      <td>0.492508</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>275.01714</td>\n",
       "      <td>-10.846</td>\n",
       "      <td>0.568852</td>\n",
       "      <td>0.934398</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.813</td>\n",
       "      <td>sexual</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.873642</td>\n",
       "      <td>0.281888</td>\n",
       "      <td>97.940</td>\n",
       "      <td>0.041479</td>\n",
       "      <td>0.327251</td>\n",
       "      <td>0.264088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>234.78857</td>\n",
       "      <td>-10.682</td>\n",
       "      <td>0.224093</td>\n",
       "      <td>0.600045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.983</td>\n",
       "      <td>funky,mellow</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.117357</td>\n",
       "      <td>0.101048</td>\n",
       "      <td>129.386</td>\n",
       "      <td>0.062007</td>\n",
       "      <td>0.847661</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.88045</td>\n",
       "      <td>-15.296</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.778220</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.248</td>\n",
       "      <td>1.000</td>\n",
       "      <td>sprightly</td>\n",
       "      <td>jazz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key    energy  liveliness    tempo  speechiness  acousticness  \\\n",
       "0  8.0  0.831914    0.056928  106.245     0.232162      0.072076   \n",
       "1  1.0  0.434896    0.113409  105.276     0.071236      0.456392   \n",
       "2  8.0  0.326645    0.843938  123.949     0.492508      0.016599   \n",
       "3  7.0  0.873642    0.281888   97.940     0.041479      0.327251   \n",
       "4  5.0  0.117357    0.101048  129.386     0.062007      0.847661   \n",
       "\n",
       "   instrumentalness  time_signature  duration   loudness  valence  \\\n",
       "0          0.000000             1.0       4.0  254.60000   -5.984   \n",
       "1          0.322574             1.0       4.0  242.21333  -13.902   \n",
       "2          0.000000             0.0       4.0  275.01714  -10.846   \n",
       "3          0.264088             0.0       4.0  234.78857  -10.682   \n",
       "4          0.010635             0.0       4.0  128.88045  -15.296   \n",
       "\n",
       "   danceability      mode  time_signature_confidence  tempo_confidence  \\\n",
       "0      0.555338  0.837518                      0.555             0.408   \n",
       "1      0.290687  0.459532                      0.127             0.225   \n",
       "2      0.568852  0.934398                      0.562             0.418   \n",
       "3      0.224093  0.600045                      0.000             0.064   \n",
       "4      0.488700  0.778220                      0.369             0.529   \n",
       "\n",
       "   key_confidence  mode_confidence         moods genres  \n",
       "0           0.664            1.000  motivational    rap  \n",
       "1           0.743            0.907        classy   jazz  \n",
       "2           0.704            0.813        sexual    rap  \n",
       "3           0.167            0.983  funky,mellow    rap  \n",
       "4           0.248            1.000     sprightly   jazz  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('assignment-2_dataset.csv') # Code for generating this clean dataset is in \n",
    "                                                # `Cleaning_Song-dataset.ipynb` notebook in the \"root\" directory\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>energy</th>\n",
       "      <th>liveliness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>duration</th>\n",
       "      <th>loudness</th>\n",
       "      <th>valence</th>\n",
       "      <th>danceability</th>\n",
       "      <th>mode</th>\n",
       "      <th>time_signature_confidence</th>\n",
       "      <th>tempo_confidence</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.831914</td>\n",
       "      <td>0.056928</td>\n",
       "      <td>106.245</td>\n",
       "      <td>0.232162</td>\n",
       "      <td>0.072076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.60000</td>\n",
       "      <td>-5.984</td>\n",
       "      <td>0.555338</td>\n",
       "      <td>0.837518</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434896</td>\n",
       "      <td>0.113409</td>\n",
       "      <td>105.276</td>\n",
       "      <td>0.071236</td>\n",
       "      <td>0.456392</td>\n",
       "      <td>0.322574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>242.21333</td>\n",
       "      <td>-13.902</td>\n",
       "      <td>0.290687</td>\n",
       "      <td>0.459532</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.326645</td>\n",
       "      <td>0.843938</td>\n",
       "      <td>123.949</td>\n",
       "      <td>0.492508</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>275.01714</td>\n",
       "      <td>-10.846</td>\n",
       "      <td>0.568852</td>\n",
       "      <td>0.934398</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.873642</td>\n",
       "      <td>0.281888</td>\n",
       "      <td>97.940</td>\n",
       "      <td>0.041479</td>\n",
       "      <td>0.327251</td>\n",
       "      <td>0.264088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>234.78857</td>\n",
       "      <td>-10.682</td>\n",
       "      <td>0.224093</td>\n",
       "      <td>0.600045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.117357</td>\n",
       "      <td>0.101048</td>\n",
       "      <td>129.386</td>\n",
       "      <td>0.062007</td>\n",
       "      <td>0.847661</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.88045</td>\n",
       "      <td>-15.296</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.778220</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.248</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key    energy  liveliness    tempo  speechiness  acousticness  \\\n",
       "0  8.0  0.831914    0.056928  106.245     0.232162      0.072076   \n",
       "1  1.0  0.434896    0.113409  105.276     0.071236      0.456392   \n",
       "2  8.0  0.326645    0.843938  123.949     0.492508      0.016599   \n",
       "3  7.0  0.873642    0.281888   97.940     0.041479      0.327251   \n",
       "4  5.0  0.117357    0.101048  129.386     0.062007      0.847661   \n",
       "\n",
       "   instrumentalness  time_signature  duration   loudness  valence  \\\n",
       "0          0.000000             1.0       4.0  254.60000   -5.984   \n",
       "1          0.322574             1.0       4.0  242.21333  -13.902   \n",
       "2          0.000000             0.0       4.0  275.01714  -10.846   \n",
       "3          0.264088             0.0       4.0  234.78857  -10.682   \n",
       "4          0.010635             0.0       4.0  128.88045  -15.296   \n",
       "\n",
       "   danceability      mode  time_signature_confidence  tempo_confidence  \\\n",
       "0      0.555338  0.837518                      0.555             0.408   \n",
       "1      0.290687  0.459532                      0.127             0.225   \n",
       "2      0.568852  0.934398                      0.562             0.418   \n",
       "3      0.224093  0.600045                      0.000             0.064   \n",
       "4      0.488700  0.778220                      0.369             0.529   \n",
       "\n",
       "   key_confidence  mode_confidence  \n",
       "0           0.664            1.000  \n",
       "1           0.743            0.907  \n",
       "2           0.704            0.813  \n",
       "3           0.167            0.983  \n",
       "4           0.248            1.000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df = dataset.drop(['moods', 'genres'], axis = 1)\n",
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rap', 'jazz', 'rap', ..., 'rock', 'jazz', 'rap'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = np.ravel(dataset['genres'])\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74289754,  0.79038307, -0.82994424, ..., -0.22933918,\n",
       "         0.57256136,  0.54444631],\n",
       "       [-1.20928269, -0.79134335, -0.53280988, ..., -1.25128013,\n",
       "         0.83133061,  0.07421993],\n",
       "       [ 0.74289754, -1.22261716,  3.31034567, ..., -0.17349542,\n",
       "         0.70358377, -0.40106266],\n",
       "       ..., \n",
       "       [-1.48816558, -1.37798984, -0.56033956, ...,  0.98805495,\n",
       "        -0.66232481,  0.54444631],\n",
       "       [-0.37263402,  0.04421256, -0.65962109, ..., -1.30712389,\n",
       "         0.76254385,  0.54444631],\n",
       "       [ 0.74289754,  0.17007961,  1.7506704 , ..., -0.54764866,\n",
       "        -0.37079996, -0.2999387 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "audio_scaled = scaler.fit_transform(audio_df)\n",
    "audio_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(audio_scaled, genres, test_size =  0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build original classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_classifiers = {'logistic regression': LogisticRegression(solver = 'saga', multi_class = 'multinomial'),\n",
    "                      'svm': SVC(C = 1, gamma = 1),\n",
    "                      'rfc': RandomForestClassifier(n_estimators = 5, min_samples_split = 2, max_features = 'log2')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logistic regression\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<349> 10  67  46 |\n",
      " jazz |  15<393> 14  34 |\n",
      "  rap |  65  19<328> 17 |\n",
      " rock |  38  56  23<326>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.75      0.74      0.74       472\n",
      "       jazz       0.82      0.86      0.84       456\n",
      "        rap       0.76      0.76      0.76       429\n",
      "       rock       0.77      0.74      0.75       443\n",
      "\n",
      "avg / total       0.77      0.78      0.77      1800\n",
      "\n",
      "\n",
      "\n",
      "Original svm\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<200>167  41  64 |\n",
      " jazz |   6<431>  4  15 |\n",
      "  rap |  20 158<231> 20 |\n",
      " rock |  20 219  13<191>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.81      0.42      0.56       472\n",
      "       jazz       0.44      0.95      0.60       456\n",
      "        rap       0.80      0.54      0.64       429\n",
      "       rock       0.66      0.43      0.52       443\n",
      "\n",
      "avg / total       0.68      0.58      0.58      1800\n",
      "\n",
      "\n",
      "\n",
      "Original rfc\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<344> 17  60  51 |\n",
      " jazz |  25<377>  8  46 |\n",
      "  rap |  60  20<332> 17 |\n",
      " rock |  74  70  27<272>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.68      0.73      0.71       472\n",
      "       jazz       0.78      0.83      0.80       456\n",
      "        rap       0.78      0.77      0.78       429\n",
      "       rock       0.70      0.61      0.66       443\n",
      "\n",
      "avg / total       0.74      0.74      0.73      1800\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, classifier in original_classifiers.items():\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    prediction = classifier.predict(X_test)\n",
    "    \n",
    "    confus_mat = ConfusionMatrix(list(y_test), list(prediction))\n",
    "    class_report = classification_report(y_test, prediction)\n",
    "    \n",
    "    print('Original ' + name)\n",
    "    print('\\n')\n",
    "    print(confus_mat)\n",
    "    print(class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, all the original_classifiers have the scores below 80%, with the highest performance belongs to the Logistic Regression model (77% for precision and 78% for recall). The original SVM model has very low scores with 68% for precision and 58% for recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build best classifiers by finding the best (combination of) parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'logistic regression': \n",
    "    {\n",
    "        'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], \n",
    "        'multi_class': ['ovr', 'multinomial']\n",
    "    }, \n",
    "    'svm': \n",
    "    {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001]\n",
    "    },\n",
    "    'rfc': \n",
    "    {\n",
    "        'n_estimators': [5, 10, 100],\n",
    "        'min_samples_split': [2, 3, 4, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2', 'auto']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_classifiers = {'logistic regression': {'estimator': LogisticRegression()},\n",
    "                    'svm': {'estimator': SVC()},\n",
    "                    'rfc': {'estimator': RandomForestClassifier()}\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, param_grid in param_grids.items():\n",
    "    \n",
    "    estimator = best_classifiers[name]['estimator']\n",
    "    \n",
    "    grid = GridSearchCV(estimator, param_grid, refit = True, n_jobs = 2)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_classifiers[name]['estimator'] = grid.best_estimator_\n",
    "    best_classifiers[name]['params'] = grid.best_params_\n",
    "    best_classifiers[name]['prediction'] = grid.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My `best_classifiers` is a dict of dict storing set of data for every classifying method (logistic regression, SVM, & random forest). <br>\n",
    "Each data set then includes best estimator (with best found parameters from GridSearchCV) in `estimator`, best parameters in `params`, and prediction result from best estimator in `prediction`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic regression': {'estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'params': {'multi_class': 'ovr', 'solver': 'newton-cg'},\n",
       "  'prediction': array(['dance', 'rap', 'jazz', ..., 'jazz', 'jazz', 'rap'], dtype=object)},\n",
       " 'rfc': {'estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=10,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False),\n",
       "  'params': {'max_features': 'sqrt',\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 100},\n",
       "  'prediction': array(['dance', 'rap', 'jazz', ..., 'jazz', 'jazz', 'rap'], dtype=object)},\n",
       " 'svm': {'estimator': SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "  'params': {'C': 1, 'gamma': 0.1},\n",
       "  'prediction': array(['dance', 'rap', 'jazz', ..., 'jazz', 'jazz', 'rap'], dtype=object)}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found ...\n",
      "\n",
      "in logistic regression are: {'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
      "in svm are: {'C': 1, 'gamma': 0.1}\n",
      "in rfc are: {'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found ...\\n')\n",
    "\n",
    "for method, attribute in best_classifiers.items():\n",
    "    \n",
    "    print('in {} are: {}'.format(method, attribute['params']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` obtained from each best estimator and real target values `y_test`, confusion matrix and classification report for every method now can be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression classifier with parameters: \n",
      "{'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<340> 10  69  53 |\n",
      " jazz |  14<402>  9  31 |\n",
      "  rap |  63  18<329> 19 |\n",
      " rock |  40  61  18<324>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.74      0.72      0.73       472\n",
      "       jazz       0.82      0.88      0.85       456\n",
      "        rap       0.77      0.77      0.77       429\n",
      "       rock       0.76      0.73      0.74       443\n",
      "\n",
      "avg / total       0.77      0.78      0.77      1800\n",
      "\n",
      "\n",
      "\n",
      "Best svm classifier with parameters: \n",
      "{'C': 1, 'gamma': 0.1}\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<331> 11  75  55 |\n",
      " jazz |  13<396>  7  40 |\n",
      "  rap |  49  20<335> 25 |\n",
      " rock |  41  55  17<330>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.76      0.70      0.73       472\n",
      "       jazz       0.82      0.87      0.84       456\n",
      "        rap       0.77      0.78      0.78       429\n",
      "       rock       0.73      0.74      0.74       443\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1800\n",
      "\n",
      "\n",
      "\n",
      "Best rfc classifier with parameters: \n",
      "{'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<358> 10  55  49 |\n",
      " jazz |  10<399>  8  39 |\n",
      "  rap |  31  18<356> 24 |\n",
      " rock |  40  55   9<339>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.82      0.76      0.79       472\n",
      "       jazz       0.83      0.88      0.85       456\n",
      "        rap       0.83      0.83      0.83       429\n",
      "       rock       0.75      0.77      0.76       443\n",
      "\n",
      "avg / total       0.81      0.81      0.81      1800\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, classifier in best_classifiers.items():\n",
    "    \n",
    "    prediction = classifier['prediction']\n",
    "    \n",
    "    confus_mat = ConfusionMatrix(list(y_test), list(prediction))\n",
    "    class_report = classification_report(y_test, prediction)\n",
    "    \n",
    "    print('Best ' + name + ' classifier with parameters: ')\n",
    "    print(classifier['params'])\n",
    "    print('\\n')\n",
    "    print(confus_mat, class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "With best parameters found, scores of the \"*best*\" SVM model are higher than that of the original model by nearly 10% in precision and 20% in recall.<br>\n",
    "Also, the best parameters returned for this method is C = 1 and gamma = 0.1, which means that the only difference between the best and the original model is the decrease in gamma (by 10 times) in the best estimator.\n",
    "\n",
    "On the other hand, for other methods, all the considered parameters between the original and the best estimators are different, but little difference in prediction scores is observed.<br>\n",
    "Specifically, there are little increase in scores of best random forest model, and almost no difference in those of the best logistic regression one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build optimized classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's have a look at the feature importances and coefficients returned by `best_classifiers['rfc']['estimator']` and `best_classifiers['logistic regression']['estimator']`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           importance\n",
      "feature                              \n",
      "acousticness                 0.177294\n",
      "speechiness                  0.135340\n",
      "mode                         0.134680\n",
      "energy                       0.082129\n",
      "tempo                        0.077228\n",
      "key_confidence               0.064448\n",
      "valence                      0.059708\n",
      "instrumentalness             0.057244\n",
      "loudness                     0.056239\n",
      "danceability                 0.033583\n",
      "tempo_confidence             0.027729\n",
      "time_signature_confidence    0.027220\n",
      "mode_confidence              0.025400\n",
      "liveliness                   0.021238\n",
      "key                          0.011579\n",
      "time_signature               0.005719\n",
      "duration                     0.003223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7febda918dd8>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAGHCAYAAACwKSV9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngpmZZU4IZmYGOCGYmVnmhGBmZgD8f2SUij01ITLKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febdaa34d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfc_importances = {'feature': audio_df.columns.values,\n",
    "                   'importance': best_classifiers['rfc']['estimator'].feature_importances_\n",
    "                  }\n",
    "rfc_importances_df = pd.DataFrame(rfc_importances)\n",
    "rfc_importances_df.sort_values(by = ['importance'], ascending= False, inplace= True)\n",
    "rfc_importances_df.set_index('feature', inplace = True)\n",
    "print(rfc_importances_df)\n",
    "rfc_importances_df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058823529411764705"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(best_classifiers['rfc']['estimator'].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05696527,  0.72148895, -0.11871585,  0.35404773, -0.39757135,\n",
       "        -0.79595924,  0.46994881, -0.25206835,  0.29172998,  0.44311001,\n",
       "         0.10664753, -0.41165958,  1.15408035, -0.26681219,  0.22492041,\n",
       "         0.50832284, -0.10801094],\n",
       "       [-0.20938273, -0.8784298 ,  0.07925582, -0.0823219 , -0.83819243,\n",
       "         1.46960623,  0.19690176, -0.17440359,  0.14811786,  0.21854349,\n",
       "        -0.3956161 ,  0.86955807, -0.47283915, -0.1009225 , -0.3960657 ,\n",
       "        -0.38265274, -0.04658993],\n",
       "       [ 0.04529943, -0.42494309,  0.09172202, -0.35788793,  1.08122035,\n",
       "        -0.81640115, -0.3367475 , -0.10335043, -0.18680586, -0.15501524,\n",
       "         0.48058473,  0.06163259,  0.95507851,  0.06695219, -0.11998802,\n",
       "        -0.61139229,  0.35680049],\n",
       "       [ 0.09731279,  0.60691347,  0.0215155 ,  0.06793878, -1.08354152,\n",
       "        -0.59299192, -0.50553971,  0.4536013 , -0.04099772, -0.41453233,\n",
       "        -0.23304072,  0.24233002, -1.58152688,  0.16301761,  0.24036245,\n",
       "         0.15184503,  0.04460509]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifiers['logistic regression']['estimator'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimized_classifiers = {'logistic regression': {},\n",
    "                         'svm': {},\n",
    "                         'rfc': {},\n",
    "                        }\n",
    "\n",
    "for key, value in optimized_classifiers.items():\n",
    "    \n",
    "    classifier = best_classifiers[key]['estimator'] # To use SelectFromModel on the prefitted best classifier\n",
    "    classifier_clone = clone(classifier) # So that in later, transformed data is fitted on the estimator with \n",
    "                                            # same parameters as the best classifier\n",
    "    \n",
    "    if key == 'svm':\n",
    "        value['SKB'] = SelectKBest(k = 7).fit(audio_scaled, genres)\n",
    "        value['SKB_features'] = audio_df.columns[value['SKB'].get_support(indices = True)].tolist()\n",
    "        \n",
    "        skb_X_train = X_train[:, value['SKB'].get_support()].copy()\n",
    "        skb_X_test = X_test[:, value['SKB'].get_support()].copy()\n",
    "        \n",
    "        classifier_clone.fit(skb_X_train, y_train)\n",
    "        value['SKB_prediction'] = classifier_clone.predict(skb_X_test)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        value['SFM'] = SelectFromModel(classifier, prefit = True)\n",
    "        value['SFM_features'] = audio_df.columns[value['SFM'].get_support(indices = True)].tolist()\n",
    "        \n",
    "        sfm_X_train = value['SFM'].transform(X_train.copy())\n",
    "        sfm_X_test = value['SFM'].transform(X_test.copy())\n",
    "        \n",
    "        classifier_clone.fit(sfm_X_train, y_train)\n",
    "        value['SFM_prediction'] = classifier_clone.predict(sfm_X_test)\n",
    "        \n",
    "        new_clone = clone(classifier)\n",
    "        value['RFE'] = RFE(new_clone, n_features_to_select = 7).fit(X_train, y_train)\n",
    "        value['RFE_features'] = audio_df.columns.values[value['RFE'].support_]\n",
    "        value['RFE_prediction'] = value['RFE'].predict(X_test)\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Random Forest model, I chose the default `threshold` in `SelectFromModel` is the mean in feature importances. <br>\n",
    "As from best random forest model obtained, this value is around 0.0588 and there are 7 features having importance above this, so I chose `n_features_to_select` of `RFE` to be 7 to see how things gonna be. <br>\n",
    "\n",
    "For logistic regression model, `n_features_to_select` of `RFE` was also set to be 7. As in this case, the default `threshold` in `SelectFromModel` is the mean in `coef_`, so the number of features selected by this method might not be 7.\n",
    "\n",
    "The `k` in `SelectKBest` was also set to be 7 for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized logistic regression\n",
      "\n",
      "\n",
      "Features selected by SelectFromModel:  ['energy', 'speechiness', 'acousticness', 'danceability', 'mode', 'key_confidence']\n",
      "Accuracy of optimized classifier using those features: 0.7466666666666667\n",
      "\n",
      "Features selected by RFE:  ['energy' 'speechiness' 'acousticness' 'instrumentalness' 'danceability'\n",
      " 'mode' 'key_confidence']\n",
      "Accuracy of optimized classifier using those features:  0.757222222222\n",
      "\n",
      "\n",
      "\n",
      "Optimized svm\n",
      "\n",
      "\n",
      "Features selected by SelectKBest:  ['energy', 'speechiness', 'acousticness', 'valence', 'mode', 'key_confidence', 'mode_confidence']\n",
      "Accuracy of optimized classifier using those features:  0.747222222222\n",
      "\n",
      "\n",
      "\n",
      "Optimized rfc\n",
      "\n",
      "\n",
      "Features selected by SelectFromModel:  ['energy', 'tempo', 'speechiness', 'acousticness', 'valence', 'mode', 'key_confidence']\n",
      "Accuracy of optimized classifier using those features: 0.7661111111111111\n",
      "\n",
      "Features selected by RFE:  ['energy' 'tempo' 'speechiness' 'acousticness' 'instrumentalness' 'mode'\n",
      " 'key_confidence']\n",
      "Accuracy of optimized classifier using those features:  0.782777777778\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in optimized_classifiers.items():\n",
    "    \n",
    "    print('\\nOptimized ' + key)\n",
    "    print('\\n')\n",
    "    \n",
    "    if key == 'svm':\n",
    "        print('Features selected by SelectKBest: ', value['SKB_features'])\n",
    "        print('Accuracy of optimized classifier using those features: ',\n",
    "              accuracy_score(y_test, value['SKB_prediction']))\n",
    "        print('\\n')\n",
    "    \n",
    "    else:\n",
    "        print('Features selected by SelectFromModel: ', value['SFM_features'])\n",
    "        print(\"Accuracy of optimized classifier using those features: {}\\n\".\n",
    "              format(accuracy_score(y_test, value['SFM_prediction'])))\n",
    "        \n",
    "        print('Features selected by RFE: ', value['RFE_features'])\n",
    "        print('Accuracy of optimized classifier using those features: ', \n",
    "              accuracy_score(y_test, value['RFE_prediction']))\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I expected, the number of features above the `mean` in `coef_` from logistic regression model (which is 6) is not as same as (less than) the number of features above the `mean` in `feature_importance`. <br>\n",
    "As a result, in logistic regression model, `RFE` returned more features than `SelectFromModel`. With more features, `RFE` gave higher accuracy score.\n",
    "\n",
    "In random forest model, `SelectFromModel` and `RFE` obviously returned the same number of features. Also, as I used the prefitted `best_classifier` for `SelectFromModel`, the set of features selected by this method is as predicted from the `rfc_importances_df` above. <br> \n",
    "However, using `RFE`, `instrumentalness` was selected instead of `valence`, and accuracy score is little higher than that from `SelectFromModel`. This reflects the variation in `feature_importance` resulting from recursively fitting and ranking the features. This variation, however, was very little. Looking back at the `rfc_importances_df`, we can also see that the `feature_importance` of `instrumentalness` and `valence` were initally very close to each other. <br>\n",
    "\n",
    "For all the model, with same parameters as `best_classifier` but reduced number of features, `optimized_classifier` gave lower scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look to the confusion matrix and classification report of these `optimized_classifiers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized logistic regression\n",
      "\n",
      "\n",
      "Confusion Matrix (SelectFromModel): \n",
      "       |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<334> 12  66  60 |\n",
      " jazz |  10<394> 12  40 |\n",
      "  rap |  70  23<313> 23 |\n",
      " rock |  51  74  15<303>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Classification Report (SelectFromModel): \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.72      0.71      0.71       472\n",
      "       jazz       0.78      0.86      0.82       456\n",
      "        rap       0.77      0.73      0.75       429\n",
      "       rock       0.71      0.68      0.70       443\n",
      "\n",
      "avg / total       0.75      0.75      0.75      1800\n",
      "\n",
      "Confusion Matrix (RFE): \n",
      "       |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<336> 13  67  56 |\n",
      " jazz |  14<397>  9  36 |\n",
      "  rap |  56  22<327> 24 |\n",
      " rock |  48  73  19<303>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Classification Report (RFE): \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.74      0.71      0.73       472\n",
      "       jazz       0.79      0.87      0.83       456\n",
      "        rap       0.77      0.76      0.77       429\n",
      "       rock       0.72      0.68      0.70       443\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimized svm\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      "       |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<317> 12  76  67 |\n",
      " jazz |  10<402>  7  37 |\n",
      "  rap |  49  23<341> 16 |\n",
      " rock |  69  75  14<285>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.71      0.67      0.69       472\n",
      "       jazz       0.79      0.88      0.83       456\n",
      "        rap       0.78      0.79      0.79       429\n",
      "       rock       0.70      0.64      0.67       443\n",
      "\n",
      "avg / total       0.74      0.75      0.74      1800\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimized rfc\n",
      "\n",
      "\n",
      "Confusion Matrix (SelectFromModel): \n",
      "       |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<329> 13  68  62 |\n",
      " jazz |   8<397>  5  46 |\n",
      "  rap |  42  21<342> 24 |\n",
      " rock |  46  71  15<311>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Classification Report (SelectFromModel): \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.77      0.70      0.73       472\n",
      "       jazz       0.79      0.87      0.83       456\n",
      "        rap       0.80      0.80      0.80       429\n",
      "       rock       0.70      0.70      0.70       443\n",
      "\n",
      "avg / total       0.77      0.77      0.76      1800\n",
      "\n",
      "Confusion Matrix (RFE): \n",
      "       |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<339> 13  58  62 |\n",
      " jazz |  11<400>  9  36 |\n",
      "  rap |  35  24<348> 22 |\n",
      " rock |  45  61  15<322>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Classification Report (RFE): \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.79      0.72      0.75       472\n",
      "       jazz       0.80      0.88      0.84       456\n",
      "        rap       0.81      0.81      0.81       429\n",
      "       rock       0.73      0.73      0.73       443\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1800\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in optimized_classifiers.items():\n",
    "    \n",
    "    print('\\nOptimized ' + key)\n",
    "    print('\\n')\n",
    "    \n",
    "    if key == 'svm':\n",
    "        print('Confusion Matrix: \\n', ConfusionMatrix(list(y_test), list(value['SKB_prediction'])))\n",
    "        print('Classification Report: \\n', classification_report(y_test, value['SKB_prediction']))\n",
    "        print('\\n')\n",
    "    \n",
    "    else:\n",
    "        print('Confusion Matrix (SelectFromModel): \\n', ConfusionMatrix(list(y_test), list(value['SFM_prediction'])))\n",
    "        print('Classification Report (SelectFromModel): \\n', classification_report(y_test, value['SFM_prediction']))\n",
    "        \n",
    "        print('Confusion Matrix (RFE): \\n', ConfusionMatrix(list(y_test), list(value['RFE_prediction'])))\n",
    "        print('Classification Report (RFE): \\n', classification_report(y_test, value['RFE_prediction']))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vary the parameters of feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs_pipes = {\n",
    "    'logistic regression': [],\n",
    "    'svm': [],\n",
    "    'rfc': []\n",
    "}\n",
    "\n",
    "for method, attribute in best_classifiers.items():\n",
    "    \n",
    "    estimator = attribute['estimator']\n",
    "    clone_estimator = clone(estimator)\n",
    "    \n",
    "    if method == 'svm':\n",
    "        \n",
    "        pipeline = Pipeline([('fs', SelectKBest()), ('clf', clone_estimator)])\n",
    "        param_grid = {\n",
    "            'fs__k': [5, 6, 7, 8, 9]\n",
    "        }\n",
    "        fs_pipes[method] = [(pipeline, param_grid)]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipeline_1 = Pipeline([('fs', SelectFromModel(estimator)), ('clf', clone_estimator)])\n",
    "        \n",
    "        if method == 'logistic regression': \n",
    "            thresholds = [-0.5, -0.02, 0.03, 0.15, 0.5]\n",
    "        else:\n",
    "            thresholds = [0.01, 0.03, 0.06, 0.1]\n",
    "        param_grid_1 = {\n",
    "                'fs__threshold': thresholds\n",
    "        }\n",
    "        \n",
    "        pipeline_2 = Pipeline([('fs', RFE(clone_estimator)), ('clf', clone_estimator)])\n",
    "        param_grid_2 = {\n",
    "            'fs__n_features_to_select': [5, 7, 9, 11, 13]\n",
    "        }\n",
    "        fs_pipes[method] = [ (pipeline_1, param_grid_1), (pipeline_2, param_grid_2) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('fs', SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\n",
      "\n",
      "{'fs__threshold': [0.01, 0.03, 0.06, 0.1]}\n",
      "\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('fs', RFE(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=10,\n",
      "        ...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\n",
      "\n",
      "{'fs__n_features_to_select': [5, 7, 9, 11, 13]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pipe, grid in fs_pipes['rfc']:\n",
    "    \n",
    "    print(pipe)\n",
    "    print('\\n')\n",
    "    print(grid)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipe_optimizing (pipes_dict, X_train, X_test, y_train, n_jobs):\n",
    "\n",
    "    optimal_estimators = {}\n",
    "    \n",
    "    for method, attribute in pipes_dict.items():\n",
    "        \n",
    "        optimal_estimators[method] = {\n",
    "            'pipelines': attribute,\n",
    "            'optimal_pipes': []\n",
    "        }\n",
    "\n",
    "        for pipe, grid in attribute:\n",
    "\n",
    "            grid_pipe = GridSearchCV(pipe, grid, n_jobs = n_jobs, return_train_score=False)\n",
    "            grid_pipe.fit(X_train, y_train)\n",
    "            prediction = grid_pipe.predict(X_test)\n",
    "\n",
    "            optimal_estimators[method]['optimal_pipes'].append({\n",
    "                'estimator': grid_pipe.best_estimator_,\n",
    "                'params': grid_pipe.best_params_,\n",
    "                'prediction': prediction,\n",
    "                'cv_results': grid_pipe.cv_results_ \n",
    "            })\n",
    "\n",
    "    return optimal_estimators\n",
    "\n",
    "def evaluate_optimal_pipe (optimized_pipes, evaluation, y_test):\n",
    "    \n",
    "    for method, attribute in optimized_pipes.items():\n",
    "    \n",
    "        print(method)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "        for optimal_pipe in attribute['optimal_pipes']:\n",
    "\n",
    "            print(optimal_pipe['params'])\n",
    "            report = evaluation(y_test, optimal_pipe['prediction'])\n",
    "            print(report)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimized_fs_pipes = pipe_optimizing(fs_pipes, X_train, X_test, y_train, n_jobs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression\n",
      "\n",
      "\n",
      "{'fs__threshold': -0.5}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.72      0.70      0.71       450\n",
      "       jazz       0.80      0.87      0.84       436\n",
      "        rap       0.77      0.77      0.77       439\n",
      "       rock       0.76      0.72      0.74       475\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "\n",
      "{'fs__n_features_to_select': 13}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.72      0.70      0.71       450\n",
      "       jazz       0.80      0.87      0.83       436\n",
      "        rap       0.77      0.77      0.77       439\n",
      "       rock       0.76      0.71      0.73       475\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "\n",
      "svm\n",
      "\n",
      "\n",
      "{'fs__k': 9}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.73      0.69      0.71       450\n",
      "       jazz       0.76      0.89      0.82       436\n",
      "        rap       0.78      0.80      0.79       439\n",
      "       rock       0.74      0.64      0.69       475\n",
      "\n",
      "avg / total       0.75      0.75      0.75      1800\n",
      "\n",
      "\n",
      "\n",
      "rfc\n",
      "\n",
      "\n",
      "{'fs__threshold': 0.01}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.77      0.70      0.73       450\n",
      "       jazz       0.79      0.88      0.83       436\n",
      "        rap       0.80      0.82      0.81       439\n",
      "       rock       0.74      0.70      0.72       475\n",
      "\n",
      "avg / total       0.77      0.78      0.77      1800\n",
      "\n",
      "\n",
      "\n",
      "{'fs__n_features_to_select': 9}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.76      0.72      0.74       450\n",
      "       jazz       0.77      0.86      0.81       436\n",
      "        rap       0.80      0.81      0.80       439\n",
      "       rock       0.72      0.67      0.70       475\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_optimal_pipe(optimized_fs_pipes, classification_report, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_fs_pipes = {\n",
    "    'logistic regression': [],\n",
    "    'svm': [],\n",
    "    'rfc': []\n",
    "}\n",
    "\n",
    "for method, attribute in best_classifiers.items():\n",
    "    \n",
    "    estimator = attribute['estimator']\n",
    "    clone_estimator = clone(estimator)\n",
    "    \n",
    "    if method == 'svm':\n",
    "        \n",
    "        pipeline = Pipeline([('fs', SelectKBest()), ('clf', clone_estimator)])\n",
    "        param_grid = {\n",
    "            'fs__k': [6, 9, 10, 13] # Added 5 & 10\n",
    "        }\n",
    "        fs_pipes[method] = [(pipeline, param_grid)]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipeline_1 = Pipeline([('fs', SelectFromModel(estimator)), ('clf', clone_estimator)])\n",
    "        \n",
    "        if method == 'logistic regression': \n",
    "            thresholds = [-1, -0.7, -0.5, 0.15, 0.5] # Added -1 & -0.7\n",
    "        else:\n",
    "            thresholds = [0.005, 0.01, 0.03, 0.06] # Added 0.005\n",
    "        param_grid_1 = {\n",
    "                'fs__threshold': thresholds\n",
    "        }\n",
    "        \n",
    "        pipeline_2 = Pipeline([('fs', RFE(clone_estimator)), ('clf', clone_estimator)])\n",
    "        param_grid_2 = {\n",
    "            'fs__n_features_to_select': [9, 13, 15] # Kept 9, 13. Added 15\n",
    "        }\n",
    "        fs_pipes[method] = [ (pipeline_1, param_grid_1), (pipeline_2, param_grid_2) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_optimized_fs_pipes = pipe_optimizing(fs_pipes, X_train, X_test, y_train, n_jobs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression\n",
      "\n",
      "\n",
      "{'fs__threshold': -1}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.72      0.70      0.71       450\n",
      "       jazz       0.80      0.87      0.84       436\n",
      "        rap       0.77      0.77      0.77       439\n",
      "       rock       0.76      0.72      0.74       475\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "\n",
      "{'fs__n_features_to_select': 15}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.72      0.69      0.70       450\n",
      "       jazz       0.80      0.87      0.83       436\n",
      "        rap       0.77      0.77      0.77       439\n",
      "       rock       0.76      0.72      0.74       475\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "\n",
      "svm\n",
      "\n",
      "\n",
      "{'fs__k': 13}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.75      0.70      0.72       450\n",
      "       jazz       0.81      0.89      0.85       436\n",
      "        rap       0.78      0.77      0.78       439\n",
      "       rock       0.74      0.73      0.73       475\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1800\n",
      "\n",
      "\n",
      "\n",
      "rfc\n",
      "\n",
      "\n",
      "{'fs__threshold': 0.01}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.77      0.70      0.73       450\n",
      "       jazz       0.79      0.87      0.83       436\n",
      "        rap       0.78      0.81      0.80       439\n",
      "       rock       0.75      0.72      0.73       475\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1800\n",
      "\n",
      "\n",
      "\n",
      "{'fs__n_features_to_select': 13}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.78      0.71      0.75       450\n",
      "       jazz       0.79      0.87      0.82       436\n",
      "        rap       0.80      0.82      0.81       439\n",
      "       rock       0.74      0.71      0.72       475\n",
      "\n",
      "avg / total       0.77      0.78      0.77      1800\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_optimal_pipe(new_optimized_fs_pipes, classification_report, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the best number of features or threshold selected from `GridSearchCV` did not improve the scores.\n",
    "\n",
    "I think this possibly because that there might be cases where a parameter performs best in the \"grid search\" but fails in the whole dataset. <br>\n",
    "In fact, in the `GridSearchCV`, the test score of every parameter (used for ranking them) is from a k-fold (the default is 3-fold) cross-validation, which is performed on only 30% of our data (training data), and therefore, is very different from a cross-validation on the whole dataset.\n",
    "\n",
    "Overall, I think in this case, the original number of audio features already gave the best prediction models (with optimized model parameters) so that reducing it declined the prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine both parameter selection and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_pipe_param_grid = {\n",
    "        'fs__k': [7, 9, 13, 15],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__gamma': [1, 0.1, 0.01, 0.001]\n",
    "    }\n",
    "\n",
    "svm_pipe = {\n",
    "    'svm': [ (Pipeline([('fs', SelectKBest()), ('clf', SVC())]), svm_pipe_param_grid) ] \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_svm = pipe_optimizing(svm_pipe, X_train, X_test, y_train, n_jobs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "\n",
      "\n",
      "{'clf__C': 1, 'clf__gamma': 0.1, 'fs__k': 15}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.76      0.70      0.73       450\n",
      "       jazz       0.81      0.88      0.84       436\n",
      "        rap       0.79      0.77      0.78       439\n",
      "       rock       0.72      0.73      0.73       475\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1800\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_optimal_pipe(optimal_svm, classification_report, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No result was returned for below steps, as the computation would take a huge amount of time on my laptop. <br>\n",
    "However, the code is still here, because I think it deserves that.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiple_param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'fs__threshold': [-1, -0.7, -0.5],\n",
    "        'fs__estimator__solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], \n",
    "        'fs__estimator__multi_class': ['ovr', 'multinomial'],\n",
    "\n",
    "        'clf__solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], \n",
    "        'clf__multi_class': ['ovr', 'multinomial']\n",
    "    },\n",
    "    \n",
    "    'rfc': {\n",
    "        'fs': [SelectFromModel(RandomForestClassifier())],\n",
    "        'fs__threshold': [0.005, 0.01, 0.06],\n",
    "        'fs__estimator__n_estimators': [5, 10, 100],\n",
    "        'fs__estimator__min_samples_split': [2, 3, 4, 5, 10],\n",
    "        'fs__estimator__max_features': ['sqrt', 'log2', 'auto'],\n",
    "\n",
    "        'clf': [RandomForestClassifier()],\n",
    "        'clf__n_estimators': [5, 10, 100],\n",
    "        'clf__min_samples_split': [2, 3, 4, 5, 10],\n",
    "        'clf__max_features': ['sqrt', 'log2', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "logreg_rfc_pipes = {\n",
    "    'logistic regression': [(\n",
    "        Pipeline([('fs', SelectFromModel(LogisticRegression())), ('clf', LogisticRegression())]),\n",
    "        multiple_param_grids['logistic regression']\n",
    "    )],\n",
    "    \n",
    "    'rfc': [(\n",
    "        Pipeline([('fs', SelectFromModel(RandomForestClassifier())), ('clf', RandomForestClassifier())]),\n",
    "        multiple_param_grids['rfc']\n",
    "    )]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_logreg_rfc = pipe_optimizing(logreg_rfc_pipes, X_train, X_test, y_train, n_jobs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio features and moods\n",
    "\n",
    "** ... To be completed ... **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_nonscaled, X_test_nonscaled, \\\n",
    "y_train_nonscaled, y_test_nonscaled = train_test_split(audio_df, dataset['genres'], test_size =  0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/singuyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression classifier with parameters: \n",
      "{'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<319>  9  68  55 |\n",
      " jazz |  12<386> 12  40 |\n",
      "  rap |  65  19<333> 21 |\n",
      " rock |  46  67  17<331>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.72      0.71      0.71       451\n",
      "       jazz       0.80      0.86      0.83       450\n",
      "        rap       0.77      0.76      0.77       438\n",
      "       rock       0.74      0.72      0.73       461\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1800\n",
      "\n",
      "\n",
      "\n",
      "Best svm classifier with parameters: \n",
      "{'C': 10, 'gamma': 0.001}\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<336> 24  40  51 |\n",
      " jazz |  52<298> 45  55 |\n",
      "  rap |  66  40<262> 70 |\n",
      " rock | 110  96 100<155>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.60      0.75      0.66       451\n",
      "       jazz       0.65      0.66      0.66       450\n",
      "        rap       0.59      0.60      0.59       438\n",
      "       rock       0.47      0.34      0.39       461\n",
      "\n",
      "avg / total       0.57      0.58      0.57      1800\n",
      "\n",
      "\n",
      "\n",
      "Best rfc classifier with parameters: \n",
      "{'max_features': 'auto', 'min_samples_split': 4, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "      |   d             |\n",
      "      |   a   j       r |\n",
      "      |   n   a   r   o |\n",
      "      |   c   z   a   c |\n",
      "      |   e   z   p   k |\n",
      "------+-----------------+\n",
      "dance |<336> 11  64  40 |\n",
      " jazz |   9<386>  9  46 |\n",
      "  rap |  44  22<349> 23 |\n",
      " rock |  46  63  13<339>|\n",
      "------+-----------------+\n",
      "(row = reference; col = test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dance       0.77      0.75      0.76       451\n",
      "       jazz       0.80      0.86      0.83       450\n",
      "        rap       0.80      0.80      0.80       438\n",
      "       rock       0.76      0.74      0.75       461\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1800\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_classifiers_nonscaled = {'logistic regression': {'estimator': LogisticRegression()},\n",
    "                    'svm': {'estimator': SVC()},\n",
    "                    'rfc': {'estimator': RandomForestClassifier()}\n",
    "                   }\n",
    "\n",
    "for name, param_grid in param_grids.items():\n",
    "    \n",
    "    estimator = best_classifiers_nonscaled[name]['estimator']\n",
    "    \n",
    "    grid = GridSearchCV(estimator, param_grid, refit = True, n_jobs = 2)\n",
    "    grid.fit(X_train_nonscaled, y_train_nonscaled)\n",
    "    \n",
    "    best_classifiers_nonscaled[name]['estimator'] = grid.best_estimator_\n",
    "    best_classifiers_nonscaled[name]['params'] = grid.best_params_\n",
    "    best_classifiers_nonscaled[name]['prediction'] = grid.best_estimator_.predict(X_test_nonscaled)\n",
    "    \n",
    "for name, classifier in best_classifiers_nonscaled.items():\n",
    "    \n",
    "    prediction = classifier['prediction']\n",
    "    \n",
    "    confus_mat = ConfusionMatrix(list(y_test_nonscaled), list(prediction))\n",
    "    class_report = classification_report(y_test_nonscaled, prediction)\n",
    "    \n",
    "    print('Best ' + name + ' classifier with parameters: ')\n",
    "    print(classifier['params'])\n",
    "    print('\\n')\n",
    "    print(confus_mat, class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM, without scaling the data of many features to be in the same scale, the prediction scores drop significantly (even with best parameters). This is obvious since SVM requires data of all features to be in the same scale. \n",
    "\n",
    "For linear logistic regression, `ConvergenceWarning` was raised by `sag` solver (sag.py) of `linear_model`. <br>\n",
    "I tried increasing the `max_iter` (up to 4000) as suggested, but the problem still wasn't fixed. <br>\n",
    "I think this is because of the non-scaled data itself. In the documentation of Scikit-learn, they also stated that:\n",
    "\n",
    "> Note that sag and saga fast convergence is only guaranteed on features with approximately the same scale.\n",
    "\n",
    "[Suggestion on solving the problem](https://stats.stackexchange.com/questions/184017/how-to-fix-non-convergence-in-logisticregressioncv) <br>\n",
    "[Assumptions of linear regression](http://people.duke.edu/~rnau/testing.htm) <br>\n",
    "[Distribution of independent variables in regression models](https://www.theanalysisfactor.com/the-distribution-of-independent-variables-in-regression-models/) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
